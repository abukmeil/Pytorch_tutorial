{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 span style='color:yellow'>Pytorch training Pipeline:</h3>\n",
    "<ul>\n",
    "<li style=\"color:yellow;\"><span style=\"font-size:18px;\">Importing libraries.</span></li>\n",
    "<li style=\"color:yellow;\"><span style=\"font-size:18px;\">Design the model by specifying the number of inputs and outputs, and by crafting the forward pass with various operations and layers: Design the model (inputs, outputs, forward pass).</span></li>\n",
    "<li style=\"color:yellow;\"><span style=\"font-size:18px;\">Construct loss and optimizer.</span></li>\n",
    "<li style=\"color:yellow; font-size:18px;\">Implement the training loop:</li>\n",
    "  <ul>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Initialization: Start with initial guesses for weights.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Forward pass: Compute the predicted outputs by passing the inputs through the model.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Compute Loss: Evaluate the error of the predicted output against the actual output using the loss function.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Backward pass (Backpropagation): Compute the gradients of the loss with respect to the model parameters.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Update weights: Adjust the model parameters (weights and biases) using a predetermined optimization algorithm like Stochastic Gradient Descent (SGD) or Adam.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Evaluate: Periodically check the model performance on the validation set.</span></li>\n",
    "  <li style=\"color:yellow;\"><span style=\"font-size:18px;\">Repeat: Until the performance on the validation set stops improving or after a fixed number of iterations.</span></li>\n",
    "  </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prepreparation\n",
    "data=datasets.load_breast_cancer()\n",
    "X=data[\"data\"]\n",
    "y=data[\"target\"]\n",
    "n_samples,n_features=X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In logistic regression, it is important to scale the data\n",
    "scaler= StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)\n",
    "\n",
    "X_train=torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test=torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train=torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test=torch.from_numpy(y_test.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the machine learning tutorial, we didn't use PyTorch and therefore didn't reshape y. However, for this case, y should be reshaped from a row vector to a column vector\n",
    "\n",
    "y_train=y_train.view(y_train.shape[0],1)\n",
    "y_test=y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setting up\n",
    "# apply sigmoid function at teh end of the model\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,n_input_features, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.liner=nn.Linear(n_input_features,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_predicted= torch.sigmoid(self.liner(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model=LogisticRegression(n_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "LR=0.01\n",
    "EPOCHS=100\n",
    "loss_fn=nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|| Loss: 0.2234\n",
      "3|| Loss: 0.2214\n",
      "5|| Loss: 0.2195\n",
      "7|| Loss: 0.2177\n",
      "9|| Loss: 0.2159\n",
      "11|| Loss: 0.2141\n",
      "13|| Loss: 0.2124\n",
      "15|| Loss: 0.2107\n",
      "17|| Loss: 0.2091\n",
      "19|| Loss: 0.2075\n",
      "21|| Loss: 0.2060\n",
      "23|| Loss: 0.2044\n",
      "25|| Loss: 0.2030\n",
      "27|| Loss: 0.2015\n",
      "29|| Loss: 0.2001\n",
      "31|| Loss: 0.1988\n",
      "33|| Loss: 0.1974\n",
      "35|| Loss: 0.1961\n",
      "37|| Loss: 0.1948\n",
      "39|| Loss: 0.1936\n",
      "41|| Loss: 0.1924\n",
      "43|| Loss: 0.1912\n",
      "45|| Loss: 0.1900\n",
      "47|| Loss: 0.1888\n",
      "49|| Loss: 0.1877\n",
      "51|| Loss: 0.1866\n",
      "53|| Loss: 0.1855\n",
      "55|| Loss: 0.1844\n",
      "57|| Loss: 0.1834\n",
      "59|| Loss: 0.1824\n",
      "61|| Loss: 0.1814\n",
      "63|| Loss: 0.1804\n",
      "65|| Loss: 0.1794\n",
      "67|| Loss: 0.1785\n",
      "69|| Loss: 0.1775\n",
      "71|| Loss: 0.1766\n",
      "73|| Loss: 0.1757\n",
      "75|| Loss: 0.1748\n",
      "77|| Loss: 0.1740\n",
      "79|| Loss: 0.1731\n",
      "81|| Loss: 0.1723\n",
      "83|| Loss: 0.1715\n",
      "85|| Loss: 0.1707\n",
      "87|| Loss: 0.1699\n",
      "89|| Loss: 0.1691\n",
      "91|| Loss: 0.1683\n",
      "93|| Loss: 0.1675\n",
      "95|| Loss: 0.1668\n",
      "97|| Loss: 0.1660\n",
      "99|| Loss: 0.1653\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # Forward pass\n",
    "    y_predicted=model(X_train)\n",
    "    # Loss\n",
    "    loss=loss_fn(y_predicted,y_train)\n",
    "    #Backpropagation\n",
    "    loss.backward()\n",
    "    # Weight update\n",
    "    optimizer.step()\n",
    "    [w,b]=model.parameters()\n",
    "    \n",
    "    #The .grad attribute accumulates the gradients, so we need to clear them using the following line of code \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        print(f'{epoch+1}|| Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9298\n"
     ]
    }
   ],
   "source": [
    "# prediction:\n",
    "with torch.no_grad():\n",
    "    y_predicted=model(X_test)\n",
    "    y_predicted_cls=y_predicted.round()\n",
    "    acc= y_predicted_cls.eq(y_test).sum()/ float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc: .4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
