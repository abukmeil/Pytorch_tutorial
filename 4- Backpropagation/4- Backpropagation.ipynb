{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation \n",
    "### <span style='color:yellow'>Backpropagation is a fundamental algorithm used for training neural networks, and it's short for \"backward propagation of errors.</span>\n",
    "\n",
    "### <span style='color:yellow'>Based on calculus, backpropagation efficiently computes the gradient of the loss function, which is a measure of how wrong the network's predictions are, with respect to its weights and biases.</span>\n",
    "\n",
    "### <span style='color:yellow'> Backpropagation uses the first-order partial derivative to compute the gradient, and then updates the weights and biases to minimize the loss.</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style='color:yellow'>Let x and y be real numbers, and f and g be functions mapping from real numbers to real numbers.</span>\n",
    "\n",
    "### <span style='color:yellow'>If y = f(x) and z = g(f(x)) = g(y), then the chain rule states that:</span>\n",
    "\n",
    "$$\n",
    "\\Large{\\frac{dz}{dx}=\\frac{dz}{dy} . \\frac{dy}{dx}}\n",
    "$$\n",
    "\n",
    "<img src='chainrule.png' width=450>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>For every operation that is applied to a tensor, Pytprch create teh computational graph.</span>\n",
    "\n",
    "### <span style='color:yellow'>A computational graph is a directed acyclic graph (DAG) that represents the flow of data through a machine learning model.</span>\n",
    "\n",
    "### <span style='color:yellow'>The nodes in the graph represent mathematical operations, and the edges represent the flow of data between the operations.</span>\n",
    "\n",
    "<img src='compgraph.png' width=450>\n",
    "\n",
    "### <span style='color:yellow'>The local gradient is computed after constructing the computational graph.</span>\n",
    "\n",
    "<img src='compgraph2.png' width=450>\n",
    "\n",
    "### <span style='color:yellow'>The computational graph can contain many operations, such as multiplication, convolution, addition, etc.</span>\n",
    "\n",
    "### <span style='color:yellow'>The loss between the prediction, Z, and the desired or actual label is computed at the end.</span>\n",
    "\n",
    "### <span style='color:yellow'>The loss is measured by many different metrics, such as mean square error, cross entropy, to name a few.</span>\n",
    "\n",
    "### <span style='color:yellow'>The loss is based on the specific task that the model is trained for, i.e., if it is a classification, regression, or  segmentation task.</span>\n",
    "\n",
    "<img src='localgradients.png' width=660>\n",
    "\n",
    "\n",
    "<h3 style=\"color:yellow\">The whole procedure is summarized as follows:</h3>\n",
    "\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Forward pass:</span> to compute the loss.</li>\n",
    "<li><span style=\"color:yellow\">Local gradient computation:</span> is applied at each node.</li>\n",
    "<li><span style=\"color:yellow\">Backward pass:</span> compute the derivative of weights and biases with respect to the loss.</li>\n",
    "<li><span style=\"color:yellow\">Update rules:</span> update all weights and biases and repeat until the last epoch.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear regression computational graph</h1>\n",
    "<h3 style=\"color:yellow\">The following figure illustrates the linear regression computational graph.</h3>\n",
    "<img src='linregcomp.png' width=700>\n",
    "\n",
    "\n",
    "<h3 style=\"color:yellow\">The following figure illustrates the linear regression computational graph with with numerical  estimation.</h3>\n",
    "<img src='linregcomp1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "w=torch.tensor(1.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "y_hat=w*x\n",
    "loss=(y_hat-y)**2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss.backward() \n",
    "print(w.grad) #This is typical to the loss value appeared in the above figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
