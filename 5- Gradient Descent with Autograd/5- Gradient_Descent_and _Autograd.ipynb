{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization with automatic gradient computation \n",
    "<h3 span style='color:yellow'>This tutorial will cover the implementation of baseline linear regression optimization using the Autograd module.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>There are four different cases that can be followed towards automatic optimization.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #1 manual opertaions.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Gradient computation:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Loss computation:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Parameter updates:</span> manually.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #2 gradient computation by Autograd.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:yellow\">Loss computation:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Parameter updates:</span> manually.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #3 all operations except the prediction are manually.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:lightgreen\">Loss computation:</span> Pytorch loss.</li>\n",
    "<li><span style=\"color:lightgreen\">Parameter updates:</span> Pytorch optimizer.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "<h3 span style='color:yellow'>Case #4 automatic optimization and prediction.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:green\">Prediction:</span> Pytorch model.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:lightgreen\">Loss computation:</span> Pytorch loss.</li>\n",
    "<li><span style=\"color:lightgreen\">Parameter updates:</span> Pytorch optimizer.</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase #1:  manual operations/ steps</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"Remember, linear regression represents the linear combination of inputs and weights.\"\n",
    "\"Let's consider an example where the optimal weight for the x variable is 2.\"\n",
    "x=np.array([1,2,3,4],dtype=np.float32)\n",
    "y=np.array([2,4,6,8],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "w=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):  # forward pass\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: 0.000\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "def loss(y,y_predicted): # Considering the mean square error (loass)\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "print(f'The prediction before training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient \n",
    "# Error= 1/N(w*x  - y)**2\n",
    "#dj/dw=1/N 2x(wx-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.600, loss=30.000\n",
      "epoch 2: w=1.020, loss=14.700\n",
      "epoch 3: w=1.314, loss=7.203\n",
      "epoch 4: w=1.520, loss=3.529\n",
      "epoch 5: w=1.664, loss=1.729\n",
      "epoch 6: w=1.765, loss=0.847\n",
      "epoch 7: w=1.835, loss=0.415\n",
      "epoch 8: w=1.885, loss=0.203\n",
      "epoch 9: w=1.919, loss=0.100\n",
      "epoch 10: w=1.944, loss=0.049\n",
      "epoch 11: w=1.960, loss=0.024\n",
      "epoch 12: w=1.972, loss=0.012\n",
      "epoch 13: w=1.981, loss=0.006\n",
      "epoch 14: w=1.986, loss=0.003\n",
      "epoch 15: w=1.991, loss=0.001\n",
      "epoch 16: w=1.993, loss=0.001\n",
      "epoch 17: w=1.995, loss=0.000\n",
      "epoch 18: w=1.997, loss=0.000\n",
      "epoch 19: w=1.998, loss=0.000\n",
      "epoch 20: w=1.998, loss=0.000\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "LR=0.005\n",
    "N_ITER=20\n",
    "for epoch in range(N_ITER):\n",
    "    #prediction\n",
    "    y_pred=forward(x)\n",
    "    #loss\n",
    "    l=loss(y,y_pred)\n",
    "    #gradient\n",
    "    dw=gradient(x,y,y_pred)\n",
    "    # Weight update: go in the negative direction of the gradient\n",
    "    w-=LR*dw\n",
    "    if epoch % 1==0:   # To print information at every epoch we use %1==0, if we want to print every even (2) epoch we use % 2\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99202076, 5.99521245, 7.9936166 ])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "data=np.array([5,3,4])\n",
    "forward(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase #2: Gradient calculation using Autograd</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Model prediction\n",
    "def forward(x):  # forward pass\n",
    "    return w*x\n",
    "\n",
    "# loss\n",
    "def loss(y,y_predicted): # consider the mean square error (loss)\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "print(f'The prediction before training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.300, loss=30.000\n",
      "epoch 3: w=0.772, loss=15.660\n",
      "epoch 5: w=1.113, loss=8.175\n",
      "epoch 7: w=1.359, loss=4.267\n",
      "epoch 9: w=1.537, loss=2.228\n",
      "epoch 11: w=1.665, loss=1.163\n",
      "epoch 13: w=1.758, loss=0.607\n",
      "epoch 15: w=1.825, loss=0.317\n",
      "epoch 17: w=1.874, loss=0.165\n",
      "epoch 19: w=1.909, loss=0.086\n"
     ]
    }
   ],
   "source": [
    "LR=0.01\n",
    "N_ITER=20\n",
    "# The training procedure is similar to the above procedure, except for the gradient calculation\n",
    "for epoch in range(N_ITER):\n",
    "    # prediction\n",
    "    y_pred=forward(x)\n",
    "    # Loss\n",
    "    l=loss(y,y_pred)\n",
    "    # Gradient\n",
    "    l.backward()  # dl/dw\n",
    "\n",
    "    #Update rule: Here, we do not want 'w' to be part of gradient tracking and the computational graph.\n",
    "    with torch.no_grad():\n",
    "                w-=LR*w.grad\n",
    "    \n",
    "    # Empty the gradient: zero gradient\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.3f}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6124, 5.7674, 7.6899], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "data=torch.tensor([5,3,4])\n",
    "forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
