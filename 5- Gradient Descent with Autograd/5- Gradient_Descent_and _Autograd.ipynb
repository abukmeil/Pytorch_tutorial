{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization with automatic gradient computation \n",
    "<h3 span style='color:yellow'>This tutorial will guide you through the implementation of a basic linear regression optimization using the Autograd module.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>There are four different scenarios that can be pursued for automatic optimization.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #1 manual opertaions.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Loss computation:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Gradient computation:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Parameter updates:</span> manually.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #2 gradient computation by Autograd.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:yellow\">Loss computation:</span> manually.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:yellow\">Parameter updates:</span> manually.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>Case #3 all operations except the prediction is manually.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Prediction:</span> manually.</li>\n",
    "<li><span style=\"color:lightgreen\">Loss computation:</span> Pytorch loss.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:lightgreen\">Parameter updates:</span> Pytorch optimizer.</li>\n",
    "</ul>\n",
    "</ul>\n",
    "<h3 span style='color:yellow'>Case #4 automatic optimization and prediction.</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:lightgreen\">Prediction:</span> Pytorch model.</li>\n",
    "<li><span style=\"color:lightgreen\">Loss computation:</span> Pytorch loss.</li>\n",
    "<li><span style=\"color:lightgreen\">Gradient computation:</span> Autograd.</li>\n",
    "<li><span style=\"color:lightgreen\">Parameter updates:</span> Pytorch optimizer.</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case #1:  manual operations/ steps.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Keep in mind, linear regression signifies the linear combination of inputs and weights.\n",
    "# Consider an example in which the optimal weight for the 'x' variable is 2.\n",
    "x=np.array([1,2,3,4],dtype=np.float32)\n",
    "y=np.array([2,4,6,8],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Initialization.\n",
    "w=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):  # forward pass\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: 0.000\n"
     ]
    }
   ],
   "source": [
    "# loss\n",
    "def loss(y,y_predicted): # Considering the Mean Square Error (Loss).\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "print(f'The prediction before training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient \n",
    "# Error= 1/N(w*x  - y)**2\n",
    "#dj/dw=1/N 2x(wx-y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.600, loss=30.000\n",
      "epoch 2: w=1.020, loss=14.700\n",
      "epoch 3: w=1.314, loss=7.203\n",
      "epoch 4: w=1.520, loss=3.529\n",
      "epoch 5: w=1.664, loss=1.729\n",
      "epoch 6: w=1.765, loss=0.847\n",
      "epoch 7: w=1.835, loss=0.415\n",
      "epoch 8: w=1.885, loss=0.203\n",
      "epoch 9: w=1.919, loss=0.100\n",
      "epoch 10: w=1.944, loss=0.049\n",
      "epoch 11: w=1.960, loss=0.024\n",
      "epoch 12: w=1.972, loss=0.012\n",
      "epoch 13: w=1.981, loss=0.006\n",
      "epoch 14: w=1.986, loss=0.003\n",
      "epoch 15: w=1.991, loss=0.001\n",
      "epoch 16: w=1.993, loss=0.001\n",
      "epoch 17: w=1.995, loss=0.000\n",
      "epoch 18: w=1.997, loss=0.000\n",
      "epoch 19: w=1.998, loss=0.000\n",
      "epoch 20: w=1.998, loss=0.000\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "LR=0.005\n",
    "N_ITER=20\n",
    "\n",
    "for epoch in range(N_ITER):\n",
    "    #prediction\n",
    "    y_pred=forward(x)\n",
    "    #loss\n",
    "    l=loss(y,y_pred)\n",
    "    #gradient\n",
    "    dw=gradient(x,y,y_pred)\n",
    "    # Weight update: go in the negative direction of the gradient.\n",
    "    w-=LR*dw\n",
    "    if epoch % 1==0:   # To print information at every epoch we use %1==0, if we want to print every even (2) epoch we use % 2.\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99202076, 5.99521245, 7.9936166 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "data=np.array([5,3,4])\n",
    "forward(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case #2: Gradient calculation using Autograd</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Model prediction\n",
    "def forward(x):  # forward pass\n",
    "    return w*x\n",
    "\n",
    "# loss\n",
    "def loss(y,y_predicted): # Considering the Mean Square Error (Loss)\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "print(f'The prediction before training: {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.300, loss=30.000\n",
      "epoch 3: w=0.772, loss=15.660\n",
      "epoch 5: w=1.113, loss=8.175\n",
      "epoch 7: w=1.359, loss=4.267\n",
      "epoch 9: w=1.537, loss=2.228\n",
      "epoch 11: w=1.665, loss=1.163\n",
      "epoch 13: w=1.758, loss=0.607\n",
      "epoch 15: w=1.825, loss=0.317\n",
      "epoch 17: w=1.874, loss=0.165\n",
      "epoch 19: w=1.909, loss=0.086\n"
     ]
    }
   ],
   "source": [
    "LR=0.01\n",
    "N_ITER=20\n",
    "# The training process is similar to the aforementioned procedure, with the exception of the gradient calculation.\n",
    "for epoch in range(N_ITER):\n",
    "    # prediction\n",
    "    y_pred=forward(x)\n",
    "    # Loss\n",
    "    l=loss(y,y_pred)\n",
    "    # Gradient\n",
    "    l.backward()  # dl/dw\n",
    "    \n",
    "    # Update Rule: In this case, we don't want 'w' to be part of the gradient tracking or the computational graph.\n",
    "    with torch.no_grad():\n",
    "                w-=LR*w.grad\n",
    "    \n",
    "    # Empty the gradient: zero gradient.\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch %2==0:\n",
    "        print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.3f}')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.6124, 5.7674, 7.6899], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "data=torch.tensor([5,3,4])\n",
    "forward(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case #3: All operations, except the prediction, are performed manually.</h1>\n",
    "\n",
    "<h3 span style='color:yellow'>\"Replace the manually calculated loss and parameter updates with those computed using the loss and optimizer classes in PyTorch.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Pytorch training Pipeline:</h3>\n",
    "<ul>\n",
    "<li ><span style=\"color:yellow\">Importing libraries.</span></li>\n",
    "<li><span style=\"color:yellow\">Design the model by specifying the number of inputs and outputs, and by crafting the forward pass with various operations and layers: Design the model (inputs, outputs, forward pass).</span></li>\n",
    "<li><span style=\"color:yellow\">Construct loss and optimizer.</span></li>\n",
    "<li><span style=\"color:yellow\">Implement the training loop which includes conducting a forward pass to compute the prediction, carrying out a backward pass to compute the gradients, and continually updating the weights until convergence is achieved.</span></li>\n",
    "</ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):  # forward pass\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no need to manually define the loss, as it is defined during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.150, loss=30.000\n",
      "epoch 3: w=0.417, loss=21.963\n",
      "epoch 5: w=0.646, loss=16.079\n",
      "epoch 7: w=0.841, loss=11.771\n",
      "epoch 9: w=1.008, loss=8.618\n",
      "epoch 11: w=1.152, loss=6.309\n",
      "epoch 13: w=1.274, loss=4.619\n",
      "epoch 15: w=1.379, loss=3.381\n",
      "epoch 17: w=1.469, loss=2.475\n",
      "epoch 19: w=1.545, loss=1.812\n",
      "epoch 21: w=1.611, loss=1.327\n",
      "epoch 23: w=1.667, loss=0.971\n",
      "epoch 25: w=1.715, loss=0.711\n",
      "epoch 27: w=1.756, loss=0.521\n",
      "epoch 29: w=1.791, loss=0.381\n",
      "epoch 31: w=1.822, loss=0.279\n",
      "epoch 33: w=1.847, loss=0.204\n",
      "epoch 35: w=1.869, loss=0.150\n",
      "epoch 37: w=1.888, loss=0.109\n",
      "epoch 39: w=1.904, loss=0.080\n"
     ]
    }
   ],
   "source": [
    "# Define the loss and optimizer from nn class.\n",
    "LR=0.005\n",
    "N_ITER=40\n",
    "loss= nn.MSELoss()          # This is a callable function\n",
    "optimizer=torch.optim.SGD([w],lr=LR)\n",
    "\n",
    "for epoch in range(N_ITER):\n",
    "     # prediction\n",
    "    y_pred=forward(X)\n",
    "    # Loss\n",
    "    l=loss(y,y_pred)\n",
    "    # Gradient\n",
    "    l.backward()  # dl/dw\n",
    "    # Optimization step: weight update\n",
    "    optimizer.step()\n",
    "    # Empty the gradient: zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    if epoch %2==0:\n",
    "         print(f'epoch {epoch+1}: w={w:.3f}, loss={l:.3f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.5577, 5.7346, 7.6462], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "data=torch.tensor([5,3,4])\n",
    "forward(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Case #4 automatic optimization and prediction.</h3>\n",
    "<h3 span style='color:yellow'>Replace the manually implemented forward method with a PyTorch model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "X=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "The prediction before training: tensor([[-3.0947],\n",
      "        [-2.2554],\n",
      "        [-2.6750]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# There's no need to manually define the weight, loss, or forward method.\n",
    "# model=nn.Linear() # This represents a single linear layer, which is essentially a basic linear regression layer.\n",
    "# The above model necessitates the identification of the shapes of the inputs and outputs. Therefore, 'X' should be a 2D array where the number of samples is represented in the rows.\n",
    "\n",
    "X=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32) #4 samples with 1 feature\n",
    "y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "data_test=torch.tensor([[5],[3],[4]],dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features= X.shape\n",
    "print(n_samples,n_features)\n",
    "input_size=n_features \n",
    "output_size=n_features\n",
    "model=nn.Linear(input_size,output_size)\n",
    "print(f'The prediction before training: {model(data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=2.045, loss=0.003\n",
      "epoch 3: w=2.046, loss=0.003\n",
      "epoch 5: w=2.046, loss=0.003\n",
      "epoch 7: w=2.045, loss=0.003\n",
      "epoch 9: w=2.045, loss=0.003\n",
      "epoch 11: w=2.045, loss=0.003\n",
      "epoch 13: w=2.045, loss=0.003\n",
      "epoch 15: w=2.045, loss=0.003\n",
      "epoch 17: w=2.044, loss=0.003\n",
      "epoch 19: w=2.044, loss=0.003\n",
      "epoch 21: w=2.044, loss=0.003\n",
      "epoch 23: w=2.044, loss=0.003\n",
      "epoch 25: w=2.043, loss=0.003\n",
      "epoch 27: w=2.043, loss=0.003\n",
      "epoch 29: w=2.043, loss=0.003\n",
      "epoch 31: w=2.043, loss=0.003\n",
      "epoch 33: w=2.042, loss=0.003\n",
      "epoch 35: w=2.042, loss=0.003\n",
      "epoch 37: w=2.042, loss=0.003\n",
      "epoch 39: w=2.042, loss=0.003\n"
     ]
    }
   ],
   "source": [
    "# Define the loss and optimizer from nn class.\n",
    "LR=0.01\n",
    "N_ITER=40\n",
    "loss= nn.MSELoss()          # This is a callable function\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=LR)\n",
    "\n",
    "for epoch in range(N_ITER):\n",
    "     # prediction\n",
    "    y_pred=model(X)\n",
    "    # Loss\n",
    "    l=loss(y,y_pred)\n",
    "    # Gradient\n",
    "    l.backward()  # dl/dw\n",
    "    # Optimization step: weight update\n",
    "    optimizer.step()\n",
    "    # Empty the gradient: zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    [w,b]=  model.parameters()  # Unpacking the weights and biases. Remember, this is a list of lists for 'w' and 'b.\n",
    "    if epoch %2==0:\n",
    "         print(f'epoch {epoch+1}: w={w[0][0].item():.3f}, loss={l:.3f}')    # Unpack the first value of 'w' and use .item() to hide the tensor, thereby obtaining only the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: tensor([[10.0857],\n",
      "        [ 6.0025],\n",
      "        [ 8.0441]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'The prediction before training: {model(data_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above model, represented by nn.Linear(input_size, output_size), constitutes a single layer. This layer can be defined inside a class through inheritance, as follows:\n",
    "class LinearRegrission(nn.Module):\n",
    "    def __init__(self, input_size,output_size):\n",
    "        super(LinearRegrission,self).__init__()\n",
    "        self.lin=nn.Linear(input_size,output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model=LinearRegrission(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=1.095, loss=3.617\n",
      "epoch 3: w=1.251, loss=1.861\n",
      "epoch 5: w=1.360, loss=1.014\n",
      "epoch 7: w=1.437, loss=0.605\n",
      "epoch 9: w=1.490, loss=0.406\n",
      "epoch 11: w=1.528, loss=0.309\n",
      "epoch 13: w=1.555, loss=0.261\n",
      "epoch 15: w=1.575, loss=0.237\n",
      "epoch 17: w=1.589, loss=0.224\n",
      "epoch 19: w=1.600, loss=0.216\n",
      "epoch 21: w=1.608, loss=0.211\n",
      "epoch 23: w=1.614, loss=0.208\n",
      "epoch 25: w=1.619, loss=0.205\n",
      "epoch 27: w=1.623, loss=0.202\n",
      "epoch 29: w=1.627, loss=0.199\n",
      "epoch 31: w=1.630, loss=0.197\n",
      "epoch 33: w=1.633, loss=0.195\n",
      "epoch 35: w=1.635, loss=0.192\n",
      "epoch 37: w=1.638, loss=0.190\n",
      "epoch 39: w=1.640, loss=0.188\n"
     ]
    }
   ],
   "source": [
    "# Define the loss and optimizer from nn class\n",
    "LR=0.01\n",
    "N_ITER=40\n",
    "loss= nn.MSELoss()          # This is a callable function\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=LR)\n",
    "\n",
    "for epoch in range(N_ITER):\n",
    "     # prediction\n",
    "    y_pred=model(X)\n",
    "    # Loss\n",
    "    l=loss(y,y_pred)\n",
    "    # Gradient\n",
    "    l.backward()  # dl/dw\n",
    "    # Optimization step: weight update\n",
    "    optimizer.step()\n",
    "    # Empty the gradient: zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    [w,b]=  model.parameters()  # Unpacking the weights and biases. Remember, this is a list of lists for 'w' and 'b.\n",
    "    if epoch %2==0:\n",
    "         print(f'epoch {epoch+1}: w={w[0][0].item():.3f}, loss={l:.3f}')    # Unpack the first value of 'w' and use .item() to hide the tensor, thereby obtaining only the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction before training: tensor([[9.2597],\n",
      "        [5.9772],\n",
      "        [7.6184]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'The prediction before training: {model(data_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
