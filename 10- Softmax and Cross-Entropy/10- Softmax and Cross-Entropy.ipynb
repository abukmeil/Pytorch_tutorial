{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Softmax and Cross-Entropy</h1> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style='color:yellow'>Cross-entropy loss and the softmax function are considered two of the most used functions in neural networks for classification tasks, where both are used with Numpy data arrays and PyTorch tensors.</h3>\n",
    "\n",
    "<h3 style='color:yellow'>The softmax squashes the values of the output layer to be between 0 and 1, i.e., convert values to  probabilities.</h3>\n",
    "\n",
    "<h3 style='color:yellow'>The softmax is computed by considering the exponential of each numerical element value divided by the sum of all exponential values as follows:</h3>\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large{S(y)_i}= \\frac{e ^{y_i}}{\\Sigma e^{y_i}}\n",
    "$$\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"softmax.png\" width=400 />\n",
    "</div>\n",
    "\n",
    "<h3 style='color:yellow'>The model returns the class index associated with the highest probability, i.e., the index of 0.7, which is 0.</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65900114, 0.24243297, 0.09856589])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def soft_max(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "X=np.array([2,1,0.1])\n",
    "output=soft_max(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6590, 0.2424, 0.0986])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor= torch.from_numpy(X.astype(np.float32))\n",
    "output=torch.softmax(X_tensor,dim=0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:yellow'>The softmax output function is used for classification tasks; however, for the sake of reconstruction tasks, we do not use the softmax and instead we use the sigmoid function to squash the value between 0 and 1.</h3>\n",
    "\n",
    "<h3 style='color:yellow'>The sigmoid function is also used in binary classification tasks to squash the values between 0 and 1. Then it maps the values > 0.5 to one class and the values < 0.5 to another class.</h3>\n",
    "\n",
    "<h3 style='color:yellow'>The softmax function is commonly used in combination with the cross-entropy loss function in multi-class classification tasks. This combination is often referred to as \"softmax cross-entropy\" or \"softmax loss.\"</h3>\n",
    "\n",
    "$$\n",
    "\\Large{ {\\mathrm {Loss}} \\,=D(\\hat{Y},Y)} = - \\frac{1}{N} . \\sum Y_i \\,\\, log(\\hat{Y_i})\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    " \\mathrm{if} \\,\\, Y=[1,0,0]  \\\\ \\hat{Y}=[0.7,0.2,0.1]  \\\\\n",
    " D(Y,\\hat{Y})  \\rightarrow 0.35\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    " \\mathrm{if} \\,\\, Y=[1,0,0]  \\\\ \\hat{Y}=[0.1,0.3,0.6]  \\\\\n",
    " D(Y,\\hat{Y})  \\rightarrow 2.30\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first loss 0.035\n",
      "The second loss 0.768\n"
     ]
    }
   ],
   "source": [
    "# Numpy cross Entropy loss\n",
    "def cross_entropy(y_true,y_predicted):\n",
    "    loss= - (1/len(y_true) * np.sum(y_true * np.log(y_predicted),axis=0))\n",
    "    return loss\n",
    "\n",
    "y_true=np.array([1,0,0])  # This is one-hot  encoding\n",
    "y_pred1=np.array([0.9,0.2,0.1])\n",
    "y_pred2=np.array([0.1,0.3,0.6])\n",
    "loss1=cross_entropy(y_true,y_pred1)\n",
    "loss2=cross_entropy(y_true,y_pred2)\n",
    "print(f'The first loss {loss1:.3f}')\n",
    "print(f'The second loss {loss2:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:yellow'>PyTorch cross-entropy method embeds the softmax layer into it, thus there is no need to apply or recall the softmax in the last layer.</h3>\n",
    "\n",
    "$$\n",
    "nn.CrossEntropyLoss = nn.LogSoftymax + nn.NLLoss \\, \\mathrm{(negative \\, logliklehood \\,loss)}\n",
    "$$\n",
    "\n",
    "<h3 style='color:yellow'>When using PyTorch, Y_true should have the correct class label (not the one-hot encoding) class labels.</h3>\n",
    "\n",
    "<h3 style='color:yellow'>When using PyTorch, Y_predicted should be raw scores (logits without the need to convert them to probability), where torch can handle that.</h3>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652 |||| 1.9253969192504883\n"
     ]
    }
   ],
   "source": [
    "# Pytorch cross_entropy loss\n",
    "loss=nn.CrossEntropyLoss()\n",
    "y_true=torch.tensor([0]) # Here we insert the class label, not the one-hot encoding, as in the numpy cross-entropy\n",
    "\n",
    "# good prediction becuse we have one label and the first index is very high (n_sample x n_classes = 1 x 3)\n",
    "y_pred_good=torch.tensor([[2.0,1.0,0.1]]) # Take care about the size here; it should be an array of arrays, where it is in the size n_samples x n_classes. Because we have 1 class, we use [[]]; if we have two classes, we use [[],[]], and so on\n",
    "\n",
    "y_pred_bad=torch.tensor([[0.4,2.0,0.3]]) # Take care about the size here, it should be an arry of arry where it is in  size n_samples x n_classes. because we have 1 class we use [[]] if we have two [[],[]]\n",
    "\n",
    "loss1=loss(y_pred_good,y_true)\n",
    "loss2=loss(y_pred_bad,y_true)\n",
    "\n",
    "print(loss1.item(),'||||', loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) |||||| tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# To get the index of the class or the actual prediction\n",
    "_,prediction1=torch.max(y_pred_good,1)   # prediction1=torch.argmax(y_pred_good,1)\n",
    "_,prediction2=torch.max(y_pred_bad,1)\n",
    "print(prediction1,'||||||',prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3993692398071289 |||| 1.3299669027328491\n",
      "\n",
      "tensor([2, 0, 1]) |||||| tensor([0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch allows for multiple classes, i.e., multiclass classification or detection\n",
    "loss=nn.CrossEntropyLoss()\n",
    "y_true=torch.tensor([2,0,1]) # Here we insert the class label, not the one-hot encoding, as in the numpy cross-entropy\n",
    "\n",
    "# good prediction becuse we have one label and the first index is very high (n_sample x n_classes = 3 x 3)\n",
    "y_pred_good=torch.tensor([[0.5,1.0,2.1],[2.0,1.0,0.1],[2.0,3.0,0.1]]) # Take care about the size here; it should be an array of arrays, where it is in the size n_samples x n_classes. Because we have 3 classes, we use [[],[],[]]\n",
    "y_pred_bad=torch.tensor([[2.1,1.0,0.2],[0.3,1.0,0.1],[2.0,3.0,0.1]]) # Take care about the size here, it should be an arry of arry where it is in  size n_samples x n_classes. because we have 1 class we use [[]] if we have two [[],[]]\n",
    "\n",
    "loss1=loss(y_pred_good,y_true)\n",
    "loss2=loss(y_pred_bad,y_true)\n",
    "\n",
    "print(loss1.item(),'||||', loss2.item())\n",
    "print('')\n",
    "_,prediction1=torch.max(y_pred_good,1)\n",
    "_,prediction2=torch.max(y_pred_bad,1)\n",
    "print(prediction1,'||||||',prediction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:yellow'>The following figure shows a complete example of integrating the Softmax with neural networks.</h3>\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"nnn_softmax.png\" width=500 />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines of code are incolpme and are used just for he illustration and showing the concept of using softmax-loss \n",
    "class NNET(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes, *args, **kwargs):\n",
    "        super(NNET,self).__init__(*args, **kwargs)\n",
    "        self.linear=nn.Linear(input_size,hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.liner2=nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=self.linear(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.liner2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_= NNET(input_size=28*28,hidden_size=5,num_classes=3)\n",
    "loss=nn.CrossEntropyLoss() # This applies also softmax\n",
    "# We complete the other requirements here, including LR, EPOCH, optimizer, and training loop, and that is done once the data is available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
