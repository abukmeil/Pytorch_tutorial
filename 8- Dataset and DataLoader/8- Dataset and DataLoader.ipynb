{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset and DataLoader classes</h1> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 span style='color:yellow'>Calculating the gradient of samples of data can be time-consuming, especially if the data set is relatively large.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>To manage the large dataset, a useful practice is to divide the whole dataset into batches of samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Accordingly, the gradient is calculated over each epoch, and within each epoch, there is another loop that iterates over each batch of data samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> One Epoch (epoch=1) means one forward and backward pass across all training samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The batch size is the number of training samples in one forward and backward pass.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The number of iterations is the count of forward and backward passes, where each pass uses the batch size as a number of samples.</h3>\n",
    "\n",
    "<ul>\n",
    "  <li style=\"color:lightgreen;\"><span style=\"font-size:18px;\">e.g. if we have 200 samples, and the batch size is 20, then 200/20 = 10 iterations are needed to complete one epoch.</span></li>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>The PyTorch datasets and dataloader classes are introduced to automate the process of batch calculation and iterations.</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional method for reading a dataset\n",
    "import numpy as np\n",
    " \n",
    "# Columns name: Date,Open,High,Low,Close,Volume,Adj\"\n",
    "data=np.loadtxt(\"google_stock_example.csv\", delimiter=',', skiprows=1, usecols=(1, 2,4)) # we select the numerical columns, otherwise we will have a problem with the data format\n",
    "\n",
    "#Let's assume we have 5 epochs to optimize the model parameters based on our data\n",
    "for epoch in range(5):\n",
    "    x,y,z=data.T \n",
    "\n",
    "# The above lines of code lead to a gradual approach to model optimization. Therefore, we need to divide the entire dataset into smaller batches\n",
    "\n",
    "total_batches=20\n",
    "for epoch in range(5):\n",
    "    for i in range(total_batches):\n",
    "        #xX_batch,y_batch,z_batch=\n",
    "        ...\n",
    "#The nested loop above is just for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes\n",
    "import torch,torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate synthetic data for classification purposes\n",
    "np.random.seed(1234)\n",
    "n_samples=150\n",
    "labels=np.array([0]*50+[1]*50+[2]*50)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "feature1=np.random.rand(n_samples)\n",
    "feature2=np.random.rand(n_samples)\n",
    "feature3=np.random.rand(n_samples)\n",
    "\n",
    "data=pd.DataFrame({\"label\":labels, \"feature1\":feature1,\"feature2\":feature2,\"feature3\":feature3})\n",
    "data = data[['label', 'feature1', 'feature2', 'feature3']]\n",
    "data.to_csv(\"synthetic_classification_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell above, we generated data for classification. Our goal is to load this data using NumPy,\n",
    "# taking into account that the first column contains the class labels and the header row needs to be skipped\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        Xy=np.loadtxt(\"synthetic_classification_data.csv\", delimiter=',', skiprows=1, dtype=np.float64)\n",
    "        self.X=Xy[:,1:]\n",
    "        self.y=Xy[:,[0]] # we put [0] to make it as a a 2d array of [n_samples, 1]\n",
    "        # convert to torch tensor\n",
    "        self.X=torch.from_numpy(self.X)\n",
    "        self.y=torch.from_numpy(self.y)\n",
    "        self.n_samples=Xy.shape[0]        \n",
    "        \n",
    "    def __len__(self): # This retursn the length of the dataset samples\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):  # This allows indexing\n",
    "        return self.X[index],self.y[index]    # This returns a tuple of tensors for featues and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([0.3257, 0.6295, 0.2986], dtype=torch.float64) & label: tensor([1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "first_sample=dataset[0]\n",
    "# We can unpack the tuple of tensor as follows\n",
    "features,labels=first_sample\n",
    "print(f'Features: {features} & label: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.5093, 0.3631, 0.1693],\n",
      "        [0.1715, 0.7793, 0.3455],\n",
      "        [0.8024, 0.2242, 0.1454],\n",
      "        [0.1262, 0.8854, 0.6690]], dtype=torch.float64) & label: tensor([[0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4, shuffle=True,num_workers=2) # The num_workers parameter makes the data loader much faster because it enables multiprocessing\n",
    "\n",
    "# To see how dataloader works, we convert it to anm iterobject\n",
    "dataiter=iter(dataloader)\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.5451, 0.5367, 0.5748],\n",
      "        [0.9745, 0.2859, 0.5671],\n",
      "        [0.0420, 0.1787, 0.1749],\n",
      "        [0.6124, 0.5163, 0.3927]], dtype=torch.float64) & label: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# We can print the next batch, and so on.\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sammples: 150 & Number otf iteration: 38\n"
     ]
    }
   ],
   "source": [
    "# Perform a dummy training loop\n",
    "NUM_EPOCH=5\n",
    "TOTAL_SAMPLES=len(dataset)\n",
    "NUM_ITERATION=math.ceil(TOTAL_SAMPLES/4) # math.ceil returns the samllest int that's greater or equal to the num, i.e. ceil 4.2=5\n",
    "print(f'Total sammples: {TOTAL_SAMPLES} & Number otf iteration: {NUM_ITERATION}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 2/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 3/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 5/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for i , (input,labels) in enumerate(dataloader):\n",
    "        #suppose we've already implemented the forward propagation, loss calculation, and weight update steps.\n",
    "        if (i+1) % 2 ==0:\n",
    "            print(f'epoch: {epoch+1}/{NUM_EPOCH}, step: {i+1}/{NUM_ITERATION}, input: {input.shape}')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remeber that Pytorch contains a set of datasets that can be loaded directly\n",
    "from torchvision.datasets import FashionMNIST, cifar\n",
    "from torchvision import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5))]) #( (mean), (std))\n",
    "\n",
    "# Download and load the training data\n",
    "#trainset = FashionMNIST('~/learn', download=True, train=True, transform=transform)\n",
    "#train_loader=DataLoader(trainset,batch_size=64,shuffle=True)\n",
    "\n",
    "# Download and load the testing data\n",
    "#testset = FashionMNIST('~/learn', download=True, train=False, transform=transform)\n",
    "#train_loader=DataLoader(testset,batch_size=64,shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
