{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset and DataLoader classes</h1> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 span style='color:yellow'>Calculating the gradient of samples of data can be time-consuming and cumbersome especially if the data set is relatively large.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>To manage the large dataset, a useful practice is to divide the whole dataset into batches of samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Accordingly, the gradient is calculated over each epoch, and within each epoch, there is another loop that iterates over each batch of data samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> One Epoch (epoch=1) means one forward and backward pass across all training samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The batch size is the number of training samples in one forward and backward pass.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The number of iterations is the count of forward and backward passes, where each pass uses the batch size as a number of samples.</h3>\n",
    "\n",
    "<ul>\n",
    "  <li style=\"color:lightgreen;\"><span style=\"font-size:18px;\">e.g. if we have 200 samples, and the batch size is 20, then 200/20 = 10 iterations are needed to complete one epoch.</span></li>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>The PyTorch datasets and dataloader classes are introduced to automate the process of batch calculation and iterations.</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional method for reading a dataset\n",
    "import numpy as np\n",
    " \n",
    "# Columns name: Date,Open,High,Low,Close,Volume,Adj\"\n",
    "data=np.loadtxt(\"google_stock_example.csv\", delimiter=',', skiprows=1, usecols=(1, 2,4)) # we select the numerical columns, otherwise we will have a problem with the data format\n",
    "\n",
    "#Let's assume we have 5 epochs to optimize the model parameters based on our data\n",
    "for epoch in range(5):\n",
    "    x,y,z=data.T\n",
    "\n",
    "# The above lines of code lead to a gradual approach to model optimization. Therefore, we need to divide the entire dataset into smaller batches\n",
    "\n",
    "total_batches=20\n",
    "for epoch in range(5):\n",
    "    for i in range(total_batches):\n",
    "        #x_batch,y_batch,z_batch=\n",
    "        ...\n",
    "#The nested loop above is just for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/lib/python3/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader classes\n",
    "import torch,torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate synthetic data for classification purposes\n",
    "np.random.seed(1234)\n",
    "n_samples=150\n",
    "labels=np.array([0]*50+[1]*50+[2]*50)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "feature1=np.random.rand(n_samples)\n",
    "feature2=np.random.rand(n_samples)\n",
    "feature3=np.random.rand(n_samples)\n",
    "\n",
    "data=pd.DataFrame({\"label\":labels, \"feature1\":feature1,\"feature2\":feature2,\"feature3\":feature3})\n",
    "data = data[['label', 'feature1', 'feature2', 'feature3']]\n",
    "data.to_csv(\"synthetic_classification_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell above, we generated data for classification. Our goal is to load this data using NumPy,\n",
    "# taking into account that the first column contains the class labels and the header row needs to be skipped\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        Xy=np.loadtxt(\"synthetic_classification_data.csv\", delimiter=',', skiprows=1, dtype=np.float64)\n",
    "        self.X=Xy[:,1:]\n",
    "        self.y=Xy[:,[0]] # we put [0] to make it as a a 2d array of [n_samples, 1]\n",
    "        # convert to torch tensor\n",
    "        self.X=torch.from_numpy(self.X)\n",
    "        self.y=torch.from_numpy(self.y)\n",
    "        self.n_samples=Xy.shape[0]        \n",
    "        \n",
    "    def __len__(self): # This retursn the length of the dataset samples\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):  # This allows indexing\n",
    "        return self.X[index],self.y[index]    # This returns a tuple of tensors for featues and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([0.3257, 0.6295, 0.2986], dtype=torch.float64) & label: tensor([1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "first_sample=dataset[0]\n",
    "# We can unpack the tuple of tensor as follows\n",
    "features,labels=first_sample\n",
    "print(f'Features: {features} & label: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.5093, 0.3631, 0.1693],\n",
      "        [0.1715, 0.7793, 0.3455],\n",
      "        [0.8024, 0.2242, 0.1454],\n",
      "        [0.1262, 0.8854, 0.6690]], dtype=torch.float64) & label: tensor([[0.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4, shuffle=True,num_workers=2) # The num_workers parameter makes the data loader much faster because it enables multiprocessing\n",
    "\n",
    "# To see how dataloader works, we convert it to an iterobject\n",
    "dataiter=iter(dataloader)\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.5451, 0.5367, 0.5748],\n",
      "        [0.9745, 0.2859, 0.5671],\n",
      "        [0.0420, 0.1787, 0.1749],\n",
      "        [0.6124, 0.5163, 0.3927]], dtype=torch.float64) & label: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# We can print the next batch, and so on.\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sammples: 150 & Number otf iteration: 38\n"
     ]
    }
   ],
   "source": [
    "# Perform a dummy training loop\n",
    "NUM_EPOCH=5\n",
    "TOTAL_SAMPLES=len(dataset)\n",
    "NUM_ITERATION=math.ceil(TOTAL_SAMPLES/4) # math.ceil returns the samllest int that's greater or equal to the num, i.e. ceil 4.2=5\n",
    "print(f'Total sammples: {TOTAL_SAMPLES} & Number otf iteration: {NUM_ITERATION}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 2/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 3/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 5/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for i , (input,labels) in enumerate(dataloader):\n",
    "        #suppose we've already implemented the forward propagation, loss calculation, and weight update steps.\n",
    "        if (i+1) % 2 ==0:\n",
    "            print(f'epoch: {epoch+1}/{NUM_EPOCH}, step: {i+1}/{NUM_ITERATION}, input: {input.shape}')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style='color:yellow'>PyTorch provides several sample datasets. However, in most instances, we possess our own dataset and aim to integrate it with the PyTorch model.</span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example demonstrates how to load a specific image dataset along with its associated annotations\n",
    "from skimage import io\n",
    "import os\n",
    "data_path='./datastes/image data/cats_dogs/data'\n",
    "labels_path='/home/mohanad/learn/Pytorch/8- Dataset and DataLoader/datastes/image data/cats_dogs/annotations/annotations.csv'\n",
    "\n",
    "\n",
    "class CatDog(Dataset):\n",
    "    def __init__(self, annotations, root_dir, transformations=None):\n",
    "        self.annotations=pd.read_csv(annotations)\n",
    "        self.root_dir= root_dir\n",
    "        self.transformations=transformations\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgs_path=os.path.join(self.root_dir,self.annotations.iloc[index,0]) # We use iloc to access the dataframe rows. We employ the index to switch between rows, and using '0' in iloc ensures we only view the first column.\n",
    "        images=io.imread(imgs_path)\n",
    "        labels=torch.tensor(int(self.annotations.iloc[index,1])) \n",
    "        \n",
    "        # This is an optinal step we will highlight in the next tutorials\n",
    "        if self.transformations:\n",
    "            images=self.transformations(images)\n",
    "    \n",
    "        return(images,labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[[[0.9333, 0.8980, 0.8784,  ..., 0.8471, 0.8510, 0.8588],\n",
      "          [0.9333, 0.9059, 0.8784,  ..., 0.8471, 0.8510, 0.8627],\n",
      "          [0.9373, 0.9137, 0.8706,  ..., 0.8392, 0.8510, 0.8588],\n",
      "          ...,\n",
      "          [0.4627, 0.4784, 0.4980,  ..., 0.6980, 0.6941, 0.6784],\n",
      "          [0.4510, 0.4667, 0.4863,  ..., 0.6941, 0.6784, 0.6588],\n",
      "          [0.4431, 0.4588, 0.4784,  ..., 0.6902, 0.6627, 0.6353]],\n",
      "\n",
      "         [[0.9255, 0.9059, 0.8980,  ..., 0.8510, 0.8588, 0.8667],\n",
      "          [0.9255, 0.9137, 0.8980,  ..., 0.8510, 0.8588, 0.8706],\n",
      "          [0.9255, 0.9098, 0.8902,  ..., 0.8431, 0.8588, 0.8667],\n",
      "          ...,\n",
      "          [0.4353, 0.4471, 0.4549,  ..., 0.6980, 0.6941, 0.6784],\n",
      "          [0.4275, 0.4353, 0.4431,  ..., 0.7059, 0.6902, 0.6706],\n",
      "          [0.4196, 0.4275, 0.4353,  ..., 0.7020, 0.6745, 0.6471]],\n",
      "\n",
      "         [[0.7647, 0.7216, 0.6784,  ..., 0.6784, 0.7020, 0.7098],\n",
      "          [0.7647, 0.7294, 0.6784,  ..., 0.6784, 0.7020, 0.7137],\n",
      "          [0.7569, 0.7216, 0.6784,  ..., 0.6706, 0.7020, 0.7098],\n",
      "          ...,\n",
      "          [0.2549, 0.2667, 0.2784,  ..., 0.5020, 0.4980, 0.4745],\n",
      "          [0.2392, 0.2510, 0.2627,  ..., 0.4980, 0.4824, 0.4549],\n",
      "          [0.2314, 0.2431, 0.2549,  ..., 0.4941, 0.4588, 0.4314]]],\n",
      "\n",
      "\n",
      "        [[[0.1529, 0.1529, 0.1529,  ..., 0.7333, 0.8157, 0.7961],\n",
      "          [0.1647, 0.1569, 0.1569,  ..., 0.7373, 0.7922, 0.7882],\n",
      "          [0.1608, 0.1529, 0.1569,  ..., 0.7333, 0.7725, 0.7882],\n",
      "          ...,\n",
      "          [0.1137, 0.1098, 0.0980,  ..., 0.1882, 0.1725, 0.1529],\n",
      "          [0.1333, 0.1255, 0.1137,  ..., 0.1176, 0.1412, 0.1608],\n",
      "          [0.1373, 0.1294, 0.1098,  ..., 0.1098, 0.1608, 0.1804]],\n",
      "\n",
      "         [[0.1725, 0.1725, 0.1686,  ..., 0.7255, 0.8000, 0.7765],\n",
      "          [0.1843, 0.1765, 0.1725,  ..., 0.7294, 0.7765, 0.7686],\n",
      "          [0.1804, 0.1725, 0.1725,  ..., 0.7216, 0.7569, 0.7686],\n",
      "          ...,\n",
      "          [0.1059, 0.1020, 0.0902,  ..., 0.1294, 0.1216, 0.1020],\n",
      "          [0.1137, 0.1059, 0.0941,  ..., 0.0667, 0.0902, 0.1098],\n",
      "          [0.1176, 0.1098, 0.0902,  ..., 0.0588, 0.1098, 0.1294]],\n",
      "\n",
      "         [[0.1569, 0.1569, 0.1725,  ..., 0.6431, 0.6863, 0.6510],\n",
      "          [0.1686, 0.1608, 0.1765,  ..., 0.6471, 0.6627, 0.6431],\n",
      "          [0.1647, 0.1569, 0.1765,  ..., 0.6471, 0.6510, 0.6510],\n",
      "          ...,\n",
      "          [0.1176, 0.1137, 0.1020,  ..., 0.1098, 0.0980, 0.0784],\n",
      "          [0.1294, 0.1216, 0.1098,  ..., 0.0431, 0.0588, 0.0784],\n",
      "          [0.1333, 0.1255, 0.1059,  ..., 0.0353, 0.0784, 0.0980]]],\n",
      "\n",
      "\n",
      "        [[[0.7765, 0.7882, 0.8078,  ..., 0.3647, 0.4000, 0.2549],\n",
      "          [0.7843, 0.7922, 0.8078,  ..., 0.3647, 0.3882, 0.2706],\n",
      "          [0.7922, 0.8000, 0.8118,  ..., 0.3686, 0.3333, 0.4000],\n",
      "          ...,\n",
      "          [0.7686, 0.7765, 0.7804,  ..., 0.5216, 0.5961, 0.7333],\n",
      "          [0.7647, 0.7686, 0.7765,  ..., 0.6824, 0.4980, 0.6235],\n",
      "          [0.7569, 0.7647, 0.7725,  ..., 0.6745, 0.5176, 0.5882]],\n",
      "\n",
      "         [[0.7843, 0.7961, 0.8157,  ..., 0.4157, 0.4353, 0.2902],\n",
      "          [0.7922, 0.8000, 0.8157,  ..., 0.4078, 0.4235, 0.2980],\n",
      "          [0.8000, 0.8078, 0.8196,  ..., 0.4078, 0.3529, 0.4196],\n",
      "          ...,\n",
      "          [0.7490, 0.7569, 0.7608,  ..., 0.0431, 0.0784, 0.1922],\n",
      "          [0.7451, 0.7490, 0.7569,  ..., 0.3020, 0.0549, 0.1451],\n",
      "          [0.7373, 0.7451, 0.7529,  ..., 0.3529, 0.1098, 0.1451]],\n",
      "\n",
      "         [[0.7333, 0.7451, 0.7647,  ..., 0.4471, 0.4627, 0.3176],\n",
      "          [0.7412, 0.7490, 0.7647,  ..., 0.4314, 0.4431, 0.3216],\n",
      "          [0.7490, 0.7569, 0.7686,  ..., 0.4157, 0.3647, 0.4314],\n",
      "          ...,\n",
      "          [0.6745, 0.6824, 0.6863,  ..., 0.0000, 0.0275, 0.1412],\n",
      "          [0.6706, 0.6745, 0.6824,  ..., 0.2667, 0.0000, 0.0863],\n",
      "          [0.6627, 0.6706, 0.6784,  ..., 0.3098, 0.0588, 0.0824]]],\n",
      "\n",
      "\n",
      "        [[[0.5451, 0.5647, 0.5922,  ..., 0.6039, 0.6784, 0.6667],\n",
      "          [0.5412, 0.5608, 0.5882,  ..., 0.6039, 0.6549, 0.6745],\n",
      "          [0.5490, 0.5686, 0.5922,  ..., 0.6353, 0.6471, 0.6157],\n",
      "          ...,\n",
      "          [0.2118, 0.2275, 0.1137,  ..., 0.0314, 0.0392, 0.0510],\n",
      "          [0.4863, 0.2078, 0.1412,  ..., 0.0235, 0.0314, 0.0431],\n",
      "          [0.6078, 0.2588, 0.1451,  ..., 0.0196, 0.0275, 0.0353]],\n",
      "\n",
      "         [[0.5059, 0.5255, 0.5529,  ..., 0.5647, 0.6392, 0.6314],\n",
      "          [0.5020, 0.5216, 0.5490,  ..., 0.5647, 0.6196, 0.6392],\n",
      "          [0.5059, 0.5255, 0.5490,  ..., 0.5961, 0.6118, 0.5804],\n",
      "          ...,\n",
      "          [0.1804, 0.1922, 0.0784,  ..., 0.0510, 0.0588, 0.0706],\n",
      "          [0.4431, 0.1725, 0.1059,  ..., 0.0588, 0.0510, 0.0627],\n",
      "          [0.5647, 0.2235, 0.1059,  ..., 0.0549, 0.0471, 0.0549]],\n",
      "\n",
      "         [[0.4706, 0.4902, 0.5176,  ..., 0.5569, 0.6314, 0.6118],\n",
      "          [0.4667, 0.4863, 0.5137,  ..., 0.5569, 0.6000, 0.6196],\n",
      "          [0.4824, 0.5020, 0.5255,  ..., 0.5882, 0.5922, 0.5529],\n",
      "          ...,\n",
      "          [0.1059, 0.1333, 0.0510,  ..., 0.0627, 0.0745, 0.0863],\n",
      "          [0.3647, 0.1059, 0.0706,  ..., 0.0549, 0.0627, 0.0745],\n",
      "          [0.4863, 0.1569, 0.0706,  ..., 0.0510, 0.0588, 0.0667]]]]) & label: tensor([1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataset=CatDog(annotations=labels_path,root_dir=data_path, transformations=transforms.ToTensor())\n",
    "\n",
    "first_sample=dataset[0]\n",
    "\n",
    "dalaloader= DataLoader(dataset=dataset,batch_size=4,shuffle=True,num_workers=2)\n",
    "iter_=iter(dalaloader)\n",
    "iimgaes,labels=next(iter_)\n",
    "\n",
    "print(f'Features: {iimgaes} & label: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remeber that Pytorch contains a set of datasets that can be loaded directly\n",
    "from torchvision.datasets import FashionMNIST, cifar\n",
    "from torchvision import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5))]) #( (mean), (std))\n",
    "\n",
    "# Download and load the training data\n",
    "#trainset = FashionMNIST('~/learn', download=True, train=True, transform=transform)\n",
    "#train_loader=DataLoader(trainset,batch_size=64,shuffle=True)\n",
    "\n",
    "# Download and load the testing data\n",
    "#testset = FashionMNIST('~/learn', download=True, train=False, transform=transform)\n",
    "#train_loader=DataLoader(testset,batch_size=64,shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
