{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset and DataLoader classes</h1> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 span style='color:yellow'>Calculating the gradient of samples of data can be time-consuming and cumbersome especially if the data set is relatively large.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>To manage the large dataset, a useful practice is to divide the whole dataset into batches of samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'>Accordingly, the gradient is calculated over each epoch, and within each epoch, there is another loop that iterates over each batch of data samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> One Epoch (epoch=1) means one forward and backward pass across all training samples.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The batch size is the number of training samples in one forward and backward pass.</h3>\n",
    "\n",
    "<h3 span style='color:yellow'><span style='color:lightgreen'>Note:</span> The number of iterations is the count of forward and backward passes, where each pass uses the batch size as a number of samples.</h3>\n",
    "\n",
    "<ul>\n",
    "  <li style=\"color:lightgreen;\"><span style=\"font-size:18px;\">e.g. if we have 200 samples, and the batch size is 20, then 200/20 = 10 iterations are needed to complete one epoch.</span></li>\n",
    "</ul>\n",
    "\n",
    "<h3 span style='color:yellow'>The PyTorch datasets and dataloader classes are introduced to automate the process of batch calculation and iterations.</h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional method for reading a dataset\n",
    "import numpy as np\n",
    " \n",
    "# Columns name: Date,Open,High,Low,Close,Volume,Adj\"\n",
    "data=np.loadtxt(\"google_stock_example.csv\", delimiter=',', skiprows=1, usecols=(1, 2,4)) # we select the numerical columns, otherwise we will have a problem with the data format\n",
    "\n",
    "#Let's assume we have 5 epochs to optimize the model parameters based on our data\n",
    "for epoch in range(5):\n",
    "    x,y,z=data.T\n",
    "\n",
    "# The above lines of code lead to a gradual approach to model optimization. Therefore, we need to divide the entire dataset into smaller batches\n",
    "\n",
    "total_batches=20\n",
    "for epoch in range(5):\n",
    "    for i in range(total_batches):\n",
    "        #x_batch,y_batch,z_batch=\n",
    "        ...\n",
    "#The nested loop above is just for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes\n",
    "import torch,torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate synthetic data for classification purposes\n",
    "np.random.seed(1234)\n",
    "n_samples=150\n",
    "labels=np.array([0]*50+[1]*50+[2]*50)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "feature1=np.random.rand(n_samples)\n",
    "feature2=np.random.rand(n_samples)\n",
    "feature3=np.random.rand(n_samples)\n",
    "\n",
    "data=pd.DataFrame({\"label\":labels, \"feature1\":feature1,\"feature2\":feature2,\"feature3\":feature3})\n",
    "data = data[['label', 'feature1', 'feature2', 'feature3']]\n",
    "data.to_csv(\"synthetic_classification_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell above, we generated data for classification. Our goal is to load this data using NumPy,\n",
    "# taking into account that the first column contains the class labels and the header row needs to be skipped\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        Xy=np.loadtxt(\"synthetic_classification_data.csv\", delimiter=',', skiprows=1, dtype=np.float64)\n",
    "        self.X=Xy[:,1:]\n",
    "        self.y=Xy[:,[0]] # we put [0] to make it as a a 2d array of [n_samples, 1]\n",
    "        # convert to torch tensor\n",
    "        self.X=torch.from_numpy(self.X)\n",
    "        self.y=torch.from_numpy(self.y)\n",
    "        self.n_samples=Xy.shape[0]        \n",
    "        \n",
    "    def __len__(self): # This retursn the length of the dataset samples\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):  # This allows indexing\n",
    "        return self.X[index],self.y[index]    # This returns a tuple of tensors for featues and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([0.3257, 0.6295, 0.2986], dtype=torch.float64) & label: tensor([1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset=myDataset()\n",
    "first_sample=dataset[0]\n",
    "# We can unpack the tuple of tensor as follows\n",
    "features,labels=first_sample\n",
    "print(f'Features: {features} & label: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.3063, 0.7150, 0.1311],\n",
      "        [0.6228, 0.0520, 0.1323],\n",
      "        [0.8900, 0.1576, 0.9788],\n",
      "        [0.7167, 0.6060, 0.8343]], dtype=torch.float64) & label: tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# DataLoader\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=4, shuffle=True,num_workers=2) # The num_workers parameter makes the data loader much faster because it enables multiprocessing\n",
    "\n",
    "# To see how dataloader works, we convert it to an iterobject\n",
    "dataiter=iter(dataloader)\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[0.2018, 0.7140, 0.3228],\n",
      "        [0.1262, 0.8854, 0.6690],\n",
      "        [0.6508, 0.2667, 0.8288],\n",
      "        [0.0227, 0.2260, 0.8162]], dtype=torch.float64) & label: tensor([[0.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# We can print the next batch, and so on.\n",
    "data=next(dataiter)\n",
    "# We are able to unpack the features and labels for the first batch. If you want to see the next batch, repeat the above line of code\n",
    "features,labels=data\n",
    "print(f'Features: {features} & label: {labels}') # We observe that there are 4 samples and 4 labels because we set the batch size to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sammples: 150 & Number otf iteration: 38\n"
     ]
    }
   ],
   "source": [
    "# Perform a dummy training loop\n",
    "NUM_EPOCH=5\n",
    "TOTAL_SAMPLES=len(dataset)\n",
    "NUM_ITERATION=math.ceil(TOTAL_SAMPLES/4) # math.ceil returns the samllest int that's greater or equal to the num, i.e. ceil 4.2=5\n",
    "print(f'Total sammples: {TOTAL_SAMPLES} & Number otf iteration: {NUM_ITERATION}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 1/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 2/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 2/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 3/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 3/5, step: 38/38, input: torch.Size([2, 3])\n",
      "epoch: 4/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 4/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/5, step: 2/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 4/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 6/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 8/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 10/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 12/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 14/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 16/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 18/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 20/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 22/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 24/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 26/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 28/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 30/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 32/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 34/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 36/38, input: torch.Size([4, 3])\n",
      "epoch: 5/5, step: 38/38, input: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    for i , (input,labels) in enumerate(dataloader):\n",
    "        #suppose we've already implemented the forward propagation, loss calculation, and weight update steps.\n",
    "        if (i+1) % 2 ==0:\n",
    "            print(f'epoch: {epoch+1}/{NUM_EPOCH}, step: {i+1}/{NUM_ITERATION}, input: {input.shape}')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style='color:yellow'>PyTorch provides several sample datasets. However, in most practical cases, we possess our own dataset and aim to integrate it with the PyTorch model.</span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example demonstrates how to load a specific image dataset along with its associated annotations\n",
    "from skimage import io\n",
    "import os\n",
    "data_path='./datastes/image data/cats_dogs/data'\n",
    "labels_path='/home/mohanad/learn/Pytorch/8- Dataset and DataLoader/datastes/image data/cats_dogs/annotations/annotations.csv'\n",
    "\n",
    "\n",
    "class CatDog(Dataset):\n",
    "    def __init__(self, annotations, root_dir, transformations=None):\n",
    "        self.annotations=pd.read_csv(annotations)\n",
    "        self.root_dir= root_dir\n",
    "        self.transformations=transformations\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgs_path=os.path.join(self.root_dir,self.annotations.iloc[index,0]) # We use iloc to access the dataframe rows. We employ the index to switch between rows, and using '0' in iloc ensures we only view the first column.\n",
    "        images=io.imread(imgs_path)\n",
    "        labels=torch.tensor(int(self.annotations.iloc[index,1])) \n",
    "        \n",
    "        # This is an optinal step we will highlight in the next tutorials\n",
    "        if self.transformations:\n",
    "            images=self.transformations(images)\n",
    "    \n",
    "        return(images,labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style='color:yellow'>What if we have only the data without annotation?</span></h3>\n",
    "<h3><span style='color:yellow'>We introduce a preprocessing step to check the directory of images, extract labels based on mapping, and construct the dataframe to include the image names and labels.</span></h3>\n",
    "<h3><span style='color:yellow'>Later, we build our dataset object class to facilitate further tasks.</span></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat.6.jpg',\n",
       " 'cat.4.jpg',\n",
       " 'dog.0.jpg',\n",
       " 'cat.2.jpg',\n",
       " 'cat.3.jpg',\n",
       " 'cat.1.jpg',\n",
       " 'dog.1.jpg',\n",
       " 'cat.7.jpg',\n",
       " 'cat.0.jpg',\n",
       " 'cat.5.jpg']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "'Printin the images names'\n",
    "data_dir='./datastes/image data/cats_dogs/data/'\n",
    "images_name=os.listdir(data_dir)\n",
    "images_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label the images\n",
    "class_labels=[1 if 'dog' in images_name[i] else 0 for i in range(len(images_name))]\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat.6.jpg'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe construction: it is used to store the images names and their associated labels\n",
    "label_name_df=pd.DataFrame({\"images_name\":images_name,\"class_labels\":class_labels})\n",
    "label_name_df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.6.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.4.jpg\n",
      "Image shape: (224, 224, 3) & class label: 1 & Path: ./datastes/image data/cats_dogs/data/dog.0.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.2.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.3.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.1.jpg\n",
      "Image shape: (224, 224, 3) & class label: 1 & Path: ./datastes/image data/cats_dogs/data/dog.1.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.7.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.0.jpg\n",
      "Image shape: (224, 224, 3) & class label: 0 & Path: ./datastes/image data/cats_dogs/data/cat.5.jpg\n"
     ]
    }
   ],
   "source": [
    "# Verify whether the images can be accurately read along with their corresponding labels and paths\n",
    "from skimage import io\n",
    "for i in range(len(label_name_df)):\n",
    "    image_path=os.path.join(data_dir,label_name_df.iloc[i,0])\n",
    "    image=io.imread(image_path)\n",
    "    print(f'Image shape: {image.shape} & class label: {label_name_df.iloc[i,1]} & Path: {image_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogv1(Dataset):\n",
    "    def __init__(self,label_name_df,root_dir,transformations=None):\n",
    "        self.label_name_df=label_name_df\n",
    "        self.root_dir=root_dir\n",
    "        self.transformations=transformations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(label_name_df) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path=os.path.join(self.root_dir,self.label_name_df.iloc[index,0])\n",
    "        images=io.imread(image_path)\n",
    "        labels=torch.tensor(int(self.label_name_df.iloc[index,1]))\n",
    "    \n",
    "        if self.transformations:\n",
    "            image=self.transformations(images)\n",
    "            return(image,labels)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7922, 0.7961, 0.8118,  ..., 0.9529, 0.9569, 0.9490],\n",
       "          [0.7922, 0.7961, 0.8118,  ..., 0.9569, 0.9608, 0.9529],\n",
       "          [0.7961, 0.8000, 0.8118,  ..., 0.9608, 0.9647, 0.9608],\n",
       "          ...,\n",
       "          [0.6039, 0.6078, 0.6196,  ..., 0.0118, 0.0118, 0.0118],\n",
       "          [0.6039, 0.6039, 0.6078,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.5961, 0.5961, 0.6000,  ..., 0.0078, 0.0078, 0.0078]],\n",
       " \n",
       "         [[0.6471, 0.6510, 0.6588,  ..., 0.8039, 0.7922, 0.7843],\n",
       "          [0.6471, 0.6510, 0.6588,  ..., 0.8078, 0.7961, 0.7882],\n",
       "          [0.6431, 0.6471, 0.6588,  ..., 0.8118, 0.8000, 0.7961],\n",
       "          ...,\n",
       "          [0.4824, 0.4863, 0.4902,  ..., 0.0118, 0.0118, 0.0118],\n",
       "          [0.4824, 0.4824, 0.4863,  ..., 0.0078, 0.0078, 0.0078],\n",
       "          [0.4745, 0.4745, 0.4784,  ..., 0.0078, 0.0078, 0.0078]],\n",
       " \n",
       "         [[0.3412, 0.3451, 0.3569,  ..., 0.4784, 0.4706, 0.4627],\n",
       "          [0.3412, 0.3451, 0.3569,  ..., 0.4824, 0.4745, 0.4667],\n",
       "          [0.3412, 0.3451, 0.3569,  ..., 0.4941, 0.4784, 0.4745],\n",
       "          ...,\n",
       "          [0.2196, 0.2235, 0.2196,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.2196, 0.2196, 0.2235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2118, 0.2118, 0.2157,  ..., 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_v1=CatDogv1(root_dir=data_dir,label_name_df=label_name_df, transformations=torchvision.transforms.ToTensor())\n",
    "first_sample=dataset[0]\n",
    "first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training_set: 8\n",
      "\n",
      "(tensor([[[0.5451, 0.5647, 0.5922,  ..., 0.6039, 0.6784, 0.6667],\n",
      "         [0.5412, 0.5608, 0.5882,  ..., 0.6039, 0.6549, 0.6745],\n",
      "         [0.5490, 0.5686, 0.5922,  ..., 0.6353, 0.6471, 0.6157],\n",
      "         ...,\n",
      "         [0.2118, 0.2275, 0.1137,  ..., 0.0314, 0.0392, 0.0510],\n",
      "         [0.4863, 0.2078, 0.1412,  ..., 0.0235, 0.0314, 0.0431],\n",
      "         [0.6078, 0.2588, 0.1451,  ..., 0.0196, 0.0275, 0.0353]],\n",
      "\n",
      "        [[0.5059, 0.5255, 0.5529,  ..., 0.5647, 0.6392, 0.6314],\n",
      "         [0.5020, 0.5216, 0.5490,  ..., 0.5647, 0.6196, 0.6392],\n",
      "         [0.5059, 0.5255, 0.5490,  ..., 0.5961, 0.6118, 0.5804],\n",
      "         ...,\n",
      "         [0.1804, 0.1922, 0.0784,  ..., 0.0510, 0.0588, 0.0706],\n",
      "         [0.4431, 0.1725, 0.1059,  ..., 0.0588, 0.0510, 0.0627],\n",
      "         [0.5647, 0.2235, 0.1059,  ..., 0.0549, 0.0471, 0.0549]],\n",
      "\n",
      "        [[0.4706, 0.4902, 0.5176,  ..., 0.5569, 0.6314, 0.6118],\n",
      "         [0.4667, 0.4863, 0.5137,  ..., 0.5569, 0.6000, 0.6196],\n",
      "         [0.4824, 0.5020, 0.5255,  ..., 0.5882, 0.5922, 0.5529],\n",
      "         ...,\n",
      "         [0.1059, 0.1333, 0.0510,  ..., 0.0627, 0.0745, 0.0863],\n",
      "         [0.3647, 0.1059, 0.0706,  ..., 0.0549, 0.0627, 0.0745],\n",
      "         [0.4863, 0.1569, 0.0706,  ..., 0.0510, 0.0588, 0.0667]]]), tensor(0))\n",
      "\n",
      "      Animal  Label\n",
      "0  cat.0.jpg      0\n",
      "1  cat.1.jpg      0\n",
      "2  cat.2.jpg      0\n",
      "3  cat.3.jpg      0\n",
      "4  cat.4.jpg      0\n",
      "5  cat.5.jpg      0\n",
      "6  cat.6.jpg      0\n",
      "7  cat.7.jpg      0\n",
      "8  dog.0.jpg      1\n",
      "9  dog.1.jpg      1\n",
      "[4, 6, 7, 1, 9, 8, 0, 5]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataset=CatDog(annotations=labels_path,root_dir=data_path, transformations=transforms.ToTensor())\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_percent = 0.8\n",
    "train_size = int(train_percent * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "\n",
    "# Create training and testing subsets\n",
    "training_set, testing_set=torch.utils.data.random_split(dataset,[train_size,test_size])\n",
    "\n",
    "print(f\"Length of training_set: {len(training_set)}\")\n",
    "print('')\n",
    "sample=training_set[0]\n",
    "print(sample)\n",
    "print('')\n",
    "\n",
    "# Accessing an attribute of the original dataset\n",
    "attribute = training_set.dataset.annotations\n",
    "print(attribute)\n",
    "\n",
    "# Printing indices of the training set\n",
    "indices = training_set.indices\n",
    "print(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remeber that Pytorch contains a set of datasets that can be loaded directly\n",
    "from torchvision.datasets import FashionMNIST, cifar\n",
    "from torchvision import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5))]) #( (mean), (std))\n",
    "\n",
    "# Download and load the training data\n",
    "#trainset = FashionMNIST('~/learn', download=True, train=True, transform=transform)\n",
    "#train_loader=DataLoader(trainset,batch_size=64,shuffle=True)\n",
    "\n",
    "# Download and load the testing data\n",
    "#testset = FashionMNIST('~/learn', download=True, train=False, transform=transform)\n",
    "#train_loader=DataLoader(testset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
