{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd \n",
    "### <span style='color:yellow'>Pytorch, fortunately, is equipped with the Autograd package for gradient computation.</span>\n",
    "\n",
    "### <span style='color:yellow'>This tutorial covers the basics of gradient calculation using the Autograd package.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3506, -0.4263,  0.7109])\n",
      "tensor([ 1.1131, -0.1259,  1.2902], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Gradient calculation\n",
    "import torch\n",
    "x=torch.randn(3)\n",
    "print(x)\n",
    "\n",
    "\"Suppose that later we want to calculate the gradient of a function with respect to x, thus we need to specify the argument require_grad=True.\"\n",
    "\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Whenever we perform operations with tensor x, PyTorch will create a computational graph for the gradient computation of that tensor.</span>\n",
    "$$\n",
    "\\Large{y=x+2}\n",
    "$$\n",
    "### <span style='color:yellow'>We used the tensor that requires a gradient, thus a computational graph is created as shown in the following figure.</span>\n",
    "<img src='gradient_calc.png' width='400'>\n",
    "\n",
    "### <span style='color:yellow'>The above figure summarizes the backpropagation procedure for gradient calculation.</span>\n",
    "\n",
    "### <span style='color:yellow'> The first step entails a forward pass to compute y, and because we set require_grad=True, then PyTorch will create the computational graph to compute the gradient. Also, PyTorch creates an attribute for y that can be termed as grad_fn.</span>\n",
    "\n",
    "### <span style='color:yellow'> The second step entails a backward pass in which the gradient can be calculated using the partial derivative.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1131, 1.8741, 3.2902], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#The grad_fn attribute is created for gradient calculation\n",
    "y=x+2 # \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.3824,  7.0244, 21.6509], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The type of of the grad_fn depends on the operation itself, e.g., if it is an addition, multiplication, etc.\n",
    "z=y*y*2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.0192, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# The mean grad_fn\n",
    "z=z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1508, 2.4988, 4.3869])\n"
     ]
    }
   ],
   "source": [
    "# Suppose that z is the desired output, to calculate the gradient of z with respect to x\n",
    "\n",
    "z.backward()  # dz/dx\n",
    "\n",
    "# The data x has an attribute to store the gradient termed as 'grad'\n",
    "print(x.grad)\n",
    "\n",
    "# Remember to specify require_grad=True to compute the gradient, otherwise you will face a problem. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Jaccobian product\n",
    "### <span style='color:yellow'>The backward method works in the backend through using the vector Jacobian product</span>\n",
    "\n",
    "### <span style='color:yellow'>We have the Jacobian matrix with all possible partial derivatives, then we multiply it with the gradient vector</span>\n",
    "\n",
    "### <span style='color:yellow'>We obtain the final gradient by multiplying the Jacobian matrix with the gradient vector. This concept is called the chain rule</span>\n",
    "\n",
    "### <span style='color:yellow'>Autograd applies the Jacobian-vector products as shown in the following figure</span>\n",
    "\n",
    "<img src='jaccob.png' width='800'>\n",
    "\n",
    "### <span style='color:yellow'>The above z is a scalar value, thus there is no need to add an extra argument in z.backward() to compute the gradient. However, if the result is not a scalar, then we have to specify the argument for the Jacobian vector multiplication and gradient computation</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us remove the mean operation from the above z, and check if we can compute the gradient\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "y=x+2  \n",
    "z=y*y*2\n",
    "#z.backward() \n",
    "# The above z.backward() raises RuntimeError: grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.9242e-01, 1.0720e+01, 9.0267e-03])\n"
     ]
    }
   ],
   "source": [
    "# To fix teh above issue, we have to give the gardient argument v\n",
    "v=torch.tensor([0.1,1.0,0.001],dtype=torch.float32)\n",
    "z.backward(v)  # v is the gradient argument\n",
    "print(x.grad)\n",
    "\n",
    "# In many different cases, the last operation is the sum operation, and that can aggregate the tensor, leading to converting it to scalars, leading to relaxing the demand on the gradient argument v\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient tracking prevention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5001, 0.9761, 0.8060], requires_grad=True)\n",
      "tensor([1.5001, 0.9761, 0.8060])\n"
     ]
    }
   ],
   "source": [
    "# In some cases, there are no need to calculte the gradient and update the weights\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# To do so, there are three approaches approaches:\n",
    "\n",
    "# 1- x.requires_grad_(False)\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(x) # Thsi will not show require grade attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5001, 0.9761, 0.8060])\n"
     ]
    }
   ],
   "source": [
    "# 2- x.detach(), this will create a new tensor that does not require the gardient\n",
    "y=x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5001, 2.9761, 2.8060])\n"
     ]
    }
   ],
   "source": [
    "# 3- Wrapping with (context) torch\n",
    "with torch.no_grad():\n",
    "    y=x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 10., 10., 10.])\n",
      "tensor([20., 20., 20., 20.])\n",
      "tensor([30., 30., 30., 30.])\n",
      "tensor([40., 40., 40., 40.])\n",
      "tensor([50., 50., 50., 50.])\n",
      "tensor([60., 60., 60., 60.])\n",
      "tensor([70., 70., 70., 70.])\n",
      "tensor([80., 80., 80., 80.])\n",
      "tensor([90., 90., 90., 90.])\n",
      "tensor([100., 100., 100., 100.])\n"
     ]
    }
   ],
   "source": [
    "#  When using backward(), the gradient of the tensor x is accumulated into the `.grad` attribute. In other words, the values are summed up.\n",
    "\n",
    "weights=torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model_output=(weights*10).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    # We noticed that the gradients are summed and accumulated in the `.grad` method.\n",
    "    # Therefore, we must empty the gradient as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n",
      "tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "# We noticed that the gradients are summed and accumulated in the `.grad` method.\n",
    "# Therefore, we must reset the gradient as follows:\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(10):\n",
    "    model_output=(weights*10).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the next tutorial, we will integrate Autograd with the PyTorch optimizer.\n",
    "# The optimizer is a PyTorch built-in optimization module that optimizes the gradient.\n",
    "#optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "#optimizer.step()\n",
    "#optimizer.zero_grad()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
